import cv2
import mediapipe as mp
import numpy as np
import os
import time
from PIL import ImageFont, ImageDraw, Image

import tensorflow as tf

# -----------------------
# ÏÑ§Ï†ï
# -----------------------
mp_face_detection = mp.solutions.face_detection
mp_drawing = mp.solutions.drawing_utils

save_dir = "captures"
face_dir = "faces_tflite"
os.makedirs(save_dir, exist_ok=True)
os.makedirs(face_dir, exist_ok=True)

FONT_PATH = "/System/Library/Fonts/Supplemental/AppleGothic.ttf"

# -----------------------
# Ïù¥ÎØ∏ÏßÄ Ìñ•ÏÉÅ Ìï®Ïàò
# -----------------------
def enhance_image_for_detection(image):
    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)) 
    l = clahe.apply(l)
    enhanced = cv2.merge([l, a, b])
    enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2RGB)
    enhanced = cv2.GaussianBlur(enhanced, (3, 3), 0)
    return enhanced

# -----------------------
# ÌïúÍ∏Ä ÌÖçÏä§Ìä∏ Ï∂úÎ†• Ìï®Ïàò
# -----------------------
def put_text_kor(frame, text, pos, color=(0,255,0), font_size=30):
    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    font = ImageFont.truetype(FONT_PATH, font_size)
    draw.text(pos, text, font=font, fill=color[::-1])
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

# -----------------------
# Cosine similarity
# -----------------------
def cosine_similarity(a, b):
    if np.linalg.norm(a)==0 or np.linalg.norm(b)==0:
        return 0
    return np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b))

# -----------------------
# TFLite FaceNet Î™®Îç∏ Î°úÎìú
# -----------------------
interpreter = tf.lite.Interpreter(model_path="facenet.tflite")  # Î™®Îç∏ Í≤ΩÎ°ú
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
input_shape = input_details[0]['shape'][1:3]

# -----------------------
# Îì±Î°ùÎêú ÏñºÍµ¥ Î≤°ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞
# -----------------------
def load_known_faces():
    known_embeddings = []
    known_names = []
    for file in os.listdir(face_dir):
        if file.lower().endswith((".npy")):
            emb = np.load(os.path.join(face_dir, file))
            name = os.path.splitext(file)[0]
            known_embeddings.append(emb)
            known_names.append(name)
    print(f"‚úÖ Îì±Î°ùÎêú ÏñºÍµ¥ Ïàò: {len(known_names)} ‚Üí {known_names}")
    return known_embeddings, known_names

known_embeddings, known_names = load_known_faces()

# -----------------------
# ÏñºÍµ¥ ÏûÑÎ≤†Îî© Ï∂îÏ∂ú
# -----------------------
def get_embedding(face_img):
    img = cv2.resize(face_img, tuple(input_shape))
    img = img.astype(np.float32)
    img = (img - 127.5) / 128.0  # [-1,1]
    img = np.expand_dims(img, axis=0)
    interpreter.set_tensor(input_details[0]['index'], img)
    interpreter.invoke()
    embedding = interpreter.get_tensor(output_details[0]['index'])[0]
    return embedding

# -----------------------
# Ïã§ÏãúÍ∞Ñ ÏñºÍµ¥ Ïù∏Ïãù
# -----------------------
with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.4) as face_detection:
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        for i in range(1,5):
            cap = cv2.VideoCapture(i)
            if cap.isOpened():
                print(f"Ïπ¥Î©îÎùº {i}Î≤à Ïó∞Í≤∞ ÏÑ±Í≥µ")
                break
        else:
            print("ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ïπ¥Î©îÎùº ÏóÜÏùå")
            exit()

    window_name = "Face Detection + TFLite FaceNet"
    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)

    enhancement_enabled = True
    scale_factor = 0.8
    current_scale = scale_factor

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            continue

        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        processed_image = enhance_image_for_detection(image_rgb) if enhancement_enabled else image_rgb
        results = face_detection.process(processed_image)

        detected_faces = []

        if results.detections:
            h, w, _ = frame.shape
            for i, detection in enumerate(results.detections):
                bboxC = detection.location_data.relative_bounding_box
                x_min = int(bboxC.xmin * w)
                y_min = int(bboxC.ymin * h)
                box_width = int(bboxC.width * w)
                box_height = int(bboxC.height * h)

                face_crop = frame[
                    max(0, y_min):min(h, y_min + box_height),
                    max(0, x_min):min(w, x_min + box_width)
                ]

                detected_faces.append((x_min, y_min, box_width, box_height, face_crop))

                # ÏûÑÎ≤†Îî© Ï∂îÏ∂ú
                embedding = get_embedding(face_crop)

                # Cosine similarityÎ°ú Ïù∏Ïãù
                name = "Unknown"
                confidence = 0.0
                if known_embeddings:
                    sims = [cosine_similarity(embedding, k_emb) for k_emb in known_embeddings]
                    best_idx = np.argmax(sims)
                    if sims[best_idx] > 0.5:  # Ïã†Î¢∞ÎèÑ Í∏∞Ï§Ä
                        name = known_names[best_idx]
                        confidence = sims[best_idx]

                # ÏñºÍµ¥ ÌÅ¨Í∏∞ Í∏∞Î∞ò Í±∞Î¶¨ Ï∂îÏ†ï
                face_area = box_width * box_height
                distance_estimate = "Far" if face_area < 8000 else "Medium" if face_area < 20000 else "Near"

                # Î∞ïÏä§ ÏÉâÏÉÅ
                color = (0,0,255) if distance_estimate=="Far" else (0,165,255) if distance_estimate=="Medium" else (0,255,0)
                thickness = max(2,int(current_scale*3))
                cv2.rectangle(frame, (x_min, y_min), (x_min+box_width, y_min+box_height), color, thickness)

                # Ï§ëÏïôÏ†ê ÌëúÏãú
                center_x = x_min + box_width//2
                center_y = y_min + box_height//2
                circle_radius = max(5,int(current_scale*7))
                cv2.circle(frame, (center_x, center_y), circle_radius, (0,0,255), -1)

                # Ïù¥Î¶Ñ + Ïã†Î¢∞ÎèÑ ÌëúÏãú
                label = f"{name} ({confidence*100:.1f}%) [{distance_estimate}]"
                frame = put_text_kor(frame, label, (x_min, y_min-25), color=color, font_size=int(20*current_scale))

        # ÏÉÅÌÉú ÌëúÏãú
        status_text = f"Enhancement: {'ON' if enhancement_enabled else 'OFF'} | Faces: {len(results.detections) if results.detections else 0}"
        cv2.putText(frame, status_text, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,0), 2)

        cv2.imshow(window_name, frame)
        key = cv2.waitKey(30) & 0xFF

        # -------------------------
        # ÌÇ§ Ïù¥Î≤§Ìä∏
        # -------------------------
        if key == ord('q'):
            break
        elif key == ord('c'):
            timestamp = int(time.time())
            cv2.imwrite(os.path.join(save_dir, f"frame_{timestamp}.png"), frame)
            print(f"üì∏ ÌîÑÎ†àÏûÑ Ï†ÄÏû• ÏôÑÎ£å: frame_{timestamp}.png")
        elif key == ord('e'):
            enhancement_enabled = not enhancement_enabled
            print(f"Ïù¥ÎØ∏ÏßÄ Ìñ•ÏÉÅ: {'ÌôúÏÑ±Ìôî' if enhancement_enabled else 'ÎπÑÌôúÏÑ±Ìôî'}")
        elif key == ord('r'):
            if detected_faces:
                for i,(x,y,w_,h_,crop) in enumerate(detected_faces):
                    cv2.imshow(f"Register Face {i+1}", crop)
                    new_name = input(f"Îì±Î°ùÌï† Ïù¥Î¶Ñ (Face {i+1}): ").strip()
                    if new_name:
                        emb = get_embedding(crop)
                        np.save(os.path.join(face_dir,f"{new_name}.npy"), emb)
                        print(f"‚úÖ ÏñºÍµ¥ Îì±Î°ù ÏôÑÎ£å: {new_name}")
                        known_embeddings, known_names = load_known_faces()

    cap.release()
    cv2.destroyAllWindows()
    print("ÌîÑÎ°úÍ∑∏Îû® Ï¢ÖÎ£å")
