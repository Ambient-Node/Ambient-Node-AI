{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a646f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /opt/anaconda3/lib/python3.12/site-packages (0.10.21)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (0.4.38)\n",
      "Requirement already satisfied: jaxlib in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (0.4.38)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: numpy<2 in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from jax->mediapipe) (0.5.3)\n",
      "Requirement already satisfied: opt_einsum in /opt/anaconda3/lib/python3.12/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.10 in /opt/anaconda3/lib/python3.12/site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install mediapipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f750ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "459d24af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FaceDetection', 'FaceKeyPoint', 'NamedTuple', 'SolutionBase', 'Union', '_FULL_RANGE_GRAPH_FILE_PATH', '_SHORT_RANGE_GRAPH_FILE_PATH', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'detection_pb2', 'enum', 'face_detection_pb2', 'get_key_point', 'location_data_pb2', 'np']\n",
      "['FACEMESH_CONTOURS', 'FACEMESH_FACE_OVAL', 'FACEMESH_IRISES', 'FACEMESH_LEFT_EYE', 'FACEMESH_LEFT_EYEBROW', 'FACEMESH_LEFT_IRIS', 'FACEMESH_LIPS', 'FACEMESH_NOSE', 'FACEMESH_NUM_LANDMARKS', 'FACEMESH_NUM_LANDMARKS_WITH_IRISES', 'FACEMESH_RIGHT_EYE', 'FACEMESH_RIGHT_EYEBROW', 'FACEMESH_RIGHT_IRIS', 'FACEMESH_TESSELATION', 'FaceMesh', 'NamedTuple', 'SolutionBase', '_BINARYPB_FILE_PATH', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'association_calculator_pb2', 'constant_side_packet_calculator_pb2', 'detections_to_rects_calculator_pb2', 'gate_calculator_pb2', 'image_to_tensor_calculator_pb2', 'inference_calculator_pb2', 'landmarks_refinement_calculator_pb2', 'logic_calculator_pb2', 'non_max_suppression_calculator_pb2', 'np', 'rect_transformation_calculator_pb2', 'split_vector_calculator_pb2', 'ssd_anchors_calculator_pb2', 'tensors_to_classification_calculator_pb2', 'tensors_to_detections_calculator_pb2', 'tensors_to_landmarks_calculator_pb2', 'thresholding_calculator_pb2']\n",
      "['HAND_CONNECTIONS', 'HandLandmark', 'Hands', 'NamedTuple', 'SolutionBase', '_BINARYPB_FILE_PATH', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'association_calculator_pb2', 'constant_side_packet_calculator_pb2', 'detections_to_rects_calculator_pb2', 'enum', 'gate_calculator_pb2', 'image_to_tensor_calculator_pb2', 'inference_calculator_pb2', 'logic_calculator_pb2', 'non_max_suppression_calculator_pb2', 'np', 'rect_transformation_calculator_pb2', 'split_vector_calculator_pb2', 'ssd_anchors_calculator_pb2', 'tensors_to_classification_calculator_pb2', 'tensors_to_detections_calculator_pb2', 'tensors_to_landmarks_calculator_pb2', 'thresholding_calculator_pb2']\n"
     ]
    }
   ],
   "source": [
    "from mediapipe import solutions as mp\n",
    "# ÏñºÍµ¥\n",
    "print(dir(mp.face_detection))\n",
    "# ÏñºÍµ¥ Îß§Ïâ¨\n",
    "print(dir(mp.face_mesh))\n",
    "# ÏÜê\n",
    "print(dir(mp.hands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1512d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1757311055.825306 2026061 gl_context.cc:369] GL version: 2.1 (2.1 ATI-7.0.23), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1757311055.872103 2026244 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2025-09-08 14:57:36.104 python[53725:2026061] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• ÏôÑÎ£å: capture_1757311070.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# #Face Detection Î™®Îç∏ ÏÑ§Ï†ï\n",
    "# with mp_face_detection.FaceDetection(\n",
    "#     model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
    "    \n",
    "#     #ÏõπÏ∫† Ïó¥Í∏∞\n",
    "#     cap = cv2.VideoCapture(0) \n",
    "#     if not cap.isOpened():\n",
    "#       print(\"ÏõπÏ∫†ÏùÑ Ïó¥ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "#       exit()\n",
    "#     while cap.isOpened() :\n",
    "#       success, frame = cap.read()\n",
    "#       if not success :\n",
    "#         print(\"Ïù¥ÎØ∏ÏßÄÎ•º ÏùΩÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "#         continue\n",
    "\n",
    "#       #BGR Ïù¥ÎØ∏ÏßÄÎ•º RGBÎ°ú Î≥ÄÌôò\n",
    "#       image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#       #ÏñºÍµ¥ Ïù∏Ïãù Ï≤òÎ¶¨\n",
    "#       results = face_detection.process(image)\n",
    "\n",
    "#       if results.detections:\n",
    "#         # Í∞êÏßÄÎêú Î™®Îì† ÏñºÍµ¥Ïóê ÎåÄÌï¥ Î∞òÎ≥µ\n",
    "#         for detection in results.detections:\n",
    "#           mp_drawing.draw_detection(frame, detection)\n",
    "\n",
    "#         # ÌôîÎ©¥Ïóê Í≤∞Í≥º ÌëúÏãú\n",
    "#         cv2.imshow('Face Detection', frame)\n",
    "\n",
    "#         #'q'ÎàÑÎ•¥Î©¥ Ï¢ÖÎ£å\n",
    "#         key = cv2.waitKey(30) & 0xFF \n",
    "#         if key == ord('q'):\n",
    "#           break\n",
    "#         elif key == ord('c'):\n",
    "#           timestamp = int(time.time())\n",
    "#           filename = f\"capture_{timestamp}.png\"\n",
    "#           cv2.imwrite(filename, frame)\n",
    "#           print(f\"Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• ÏôÑÎ£å: {filename}\")\n",
    "        \n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7385f183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1757490210.550291   64920 gl_context.cc:369] GL version: 2.1 (2.1 ATI-7.0.23), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1757490210.555513  109650 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2025-09-10 16:43:32.323 python[2472:64920] error messaging the mach port for IMKCFRunLoopWakeUpReliable\n"
     ]
    }
   ],
   "source": [
    "# Ï†ÄÏû• Ìè¥Îçî ÏÉùÏÑ±\n",
    "save_dir = \"captures\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# FaceDetection Ï¥àÍ∏∞Ìôî\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"ÏõπÏ∫†ÏùÑ Ïó¥ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "        exit()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Ïù¥ÎØ∏ÏßÄÎ•º ÏùΩÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "            continue\n",
    "\n",
    "        # BGR -> RGB\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # ÏñºÍµ¥ Í∞êÏßÄ\n",
    "        results = face_detection.process(image_rgb)\n",
    "\n",
    "        # ÏñºÍµ¥ Í∑∏Î¶¨Í∏∞\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                mp_drawing.draw_detection(frame, detection)\n",
    "\n",
    "        # ÌôîÎ©¥ Ï∂úÎ†•\n",
    "        cv2.imshow('Face Detection', frame)\n",
    "\n",
    "        # ÌÇ§ ÏûÖÎ†• Ï≤òÎ¶¨\n",
    "        key = cv2.waitKey(30) & 0xFF\n",
    "        if key == ord('q'):  # q ÎàÑÎ•¥Î©¥ Ï¢ÖÎ£å\n",
    "            break\n",
    "        elif key == ord('c'):  # c ÎàÑÎ•¥Î©¥ Ï∫°Ï≤ò\n",
    "            if results.detections:\n",
    "                timestamp = int(time.time())\n",
    "                h, w, _ = frame.shape\n",
    "\n",
    "                for i, detection in enumerate(results.detections):\n",
    "                    #ÏñºÍµ¥ Î∞îÏö¥Îî© Î∞ïÏä§ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "                    bboxC = detection.location_data.relative_bounding_box\n",
    "                    x_min = int(bboxC.xmin * w)\n",
    "                    y_min = int(bboxC.ymin * h)\n",
    "                    box_width = int(bboxC.width * w)\n",
    "                    box_height = int(bboxC.height * h)\n",
    "\n",
    "                    # ÏñºÍµ¥ ÏòÅÏó≠Îßå ÏûòÎùºÎÇ¥Í∏∞\n",
    "                    face_crop = frame[\n",
    "                        max(0, y_min):min(h, y_min + box_height),\n",
    "                        max(0, x_min):min(w,x_min + box_width)\n",
    "                    ]\n",
    "\n",
    "                    # ÌååÏùºÎ™Ö ÏßÄÏ†ï\n",
    "                    filename = os.path.join(save_dir, f\"face_{timestamp}_{i}.png\")\n",
    "                    cv2.imwrite(filename, face_crop)\n",
    "                    print(f\"üì∏ ÏñºÍµ¥ {i} Ï†ÄÏû• ÏôÑÎ£å: {filename}\")\n",
    "            else:\n",
    "                print(\"Ï†ÄÏû•Ìï† ÏñºÍµ¥Ïù¥ ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82393b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1757489431.866311   64920 gl_context.cc:369] GL version: 2.1 (2.1 ATI-7.0.23), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1757489431.871546   97356 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏõêÎ≥∏ ÌîÑÎ†àÏûÑ Ï†ÄÏû• ÏôÑÎ£å: captures/frame_1757489558.png\n",
      "   ÏñºÍµ¥ 0 Ï†ÄÏû• ÏôÑÎ£å: captures/face_1757489558_0.png\n",
      "   ÏñºÍµ¥ 1 Ï†ÄÏû• ÏôÑÎ£å: captures/face_1757489558_1.png\n",
      "ÏõêÎ≥∏ ÌîÑÎ†àÏûÑ Ï†ÄÏû• ÏôÑÎ£å: captures/frame_1757489562.png\n",
      "   ÏñºÍµ¥ 0 Ï†ÄÏû• ÏôÑÎ£å: captures/face_1757489562_0.png\n",
      "   ÏñºÍµ¥ 1 Ï†ÄÏû• ÏôÑÎ£å: captures/face_1757489562_1.png\n"
     ]
    }
   ],
   "source": [
    "# Ï†ÄÏû• Ìè¥Îçî ÏÉùÏÑ±\n",
    "save_dir = \"captures\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# FaceDetection Ï¥àÍ∏∞Ìôî\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"ÏõπÏ∫†ÏùÑ Ïó¥ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "        exit()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Ïù¥ÎØ∏ÏßÄÎ•º ÏùΩÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "            continue\n",
    "\n",
    "        # BGR -> RGB Î≥ÄÌôò\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # ÏñºÍµ¥ Í∞êÏßÄ\n",
    "        results = face_detection.process(image_rgb)\n",
    "\n",
    "        # ÏñºÍµ¥ Í∑∏Î¶¨Í∏∞\n",
    "        if results.detections:\n",
    "            h, w, _ = frame.shape\n",
    "            for i, detection in enumerate(results.detections):\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                x_min = int(bboxC.xmin * w)\n",
    "                y_min = int(bboxC.ymin * h)\n",
    "                box_width = int(bboxC.width * w)\n",
    "                box_height = int(bboxC.height * h)\n",
    "\n",
    "                # Ïù∏Ïãù Ïã†Î¢∞ÎèÑ(%)\n",
    "                score = detection.score[0]\n",
    "                confidence_text = f\"{int(score*100)}%\"\n",
    "\n",
    "                # ÏñºÍµ¥ Î≤àÌò∏ \n",
    "                label = f\"Person{i+1} ({confidence_text})\"\n",
    "\n",
    "                #ÏñºÍµ¥ Î∞ïÏä§ Í∑∏Î¶¨Í∏∞\n",
    "                cv2.rectangle(frame, (x_min, y_min),(x_min + box_width, y_min + box_height), (0,255,0),2)\n",
    "\n",
    "                # ÌÖçÏä§Ìä∏ Ï∂úÎ†•\n",
    "                cv2.putText(frame, label, (x_min, y_min - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0),2 )                \n",
    "                \n",
    "                # ÏΩî ÎÅù Ï¢åÌëú Íµ¨ÌïòÍ∏∞\n",
    "                keypoints = detection.location_data.relative_keypoints\n",
    "                nose_tip = keypoints[2]\n",
    "                nose_x = int(nose_tip.x * w)\n",
    "                nose_y = int(nose_tip.y * h)\n",
    "\n",
    "                # ÏΩî ÎÅù Ï∞çÍ∏∞ (Îπ®Í∞Ñ Ï†ê)\n",
    "                cv2. circle(frame, (nose_x, nose_y), 5, (0,0,255), -1)\n",
    "\n",
    "                # ÏΩî ÎÅù Ï¢åÌëú ÌÖçÏä§Ìä∏\n",
    "                cv2.putText(frame, f\"Nose ({nose_x},{nose_y})\",\n",
    "                            (nose_x + 10, nose_y), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,255), 2)\n",
    "\n",
    "        # ÌôîÎ©¥ Ï∂úÎ†•\n",
    "        cv2.imshow('Face Detection', frame)\n",
    "\n",
    "        # ÌÇ§ ÏûÖÎ†• Ï≤òÎ¶¨\n",
    "        key = cv2.waitKey(30) & 0xFF\n",
    "        if key == ord('q'):  # q ÎàÑÎ•¥Î©¥ Ï¢ÖÎ£å\n",
    "            break\n",
    "        elif key == ord('c'):  # c ÎàÑÎ•¥Î©¥ Ï∫°Ï≤ò\n",
    "            timestamp = int(time.time())\n",
    "\n",
    "            # ÏõêÎ≥∏ ÌîÑÎ†àÏûÑ Ï†ÄÏû•\n",
    "            frame_filename = os.path.join(save_dir, f\"frame_{timestamp}.png\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            print(f\"ÏõêÎ≥∏ ÌîÑÎ†àÏûÑ Ï†ÄÏû• ÏôÑÎ£å: {frame_filename}\")\n",
    "\n",
    "            # ÏñºÍµ¥ ÌÅ¨Î°≠ Ï†ÄÏû•\n",
    "            if results.detections:\n",
    "                for i, detection in enumerate(results.detections):\n",
    "                    bboxC = detection.location_data.relative_bounding_box\n",
    "                    x_min = int(bboxC.xmin * w)\n",
    "                    y_min = int(bboxC.ymin * h)\n",
    "                    box_width = int(bboxC.width * w)\n",
    "                    box_height = int(bboxC.height * h)\n",
    "\n",
    "                    face_crop = frame[\n",
    "                        max(0, y_min):min(h, y_min + box_height),\n",
    "                        max(0, x_min):min(w, x_min + box_width)\n",
    "                    ]\n",
    "\n",
    "                    face_filename = os.path.join(save_dir, f\"face_{timestamp}_{i}.png\")\n",
    "                    cv2.imwrite(face_filename, face_crop)\n",
    "                    print(f\"   ÏñºÍµ¥ {i} Ï†ÄÏû• ÏôÑÎ£å: {face_filename}\")\n",
    "            else:\n",
    "                print(\"ÏñºÍµ¥Ïù¥ Í∞êÏßÄÎêòÏßÄ ÏïäÏïÑ ÏõêÎ≥∏ ÌîÑÎ†àÏûÑÎßå Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "361f40ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 16:04:17.518 python[2472:64920] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[results Í∞ùÏ≤¥ ÏÜçÏÑ±]\n",
      "['__add__', '__class__', '__class_getitem__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__match_args__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '_asdict', '_field_defaults', '_fields', '_make', '_replace', 'count', 'detections', 'index']\n",
      "\n",
      "[Ï≤´ Î≤àÏß∏ detection ÏÜçÏÑ±]\n",
      "['AssociatedDetection', 'ByteSize', 'Clear', 'ClearExtension', 'ClearField', 'CopyFrom', 'DESCRIPTOR', 'DiscardUnknownFields', 'Extensions', 'FindInitializationErrors', 'FromString', 'HasExtension', 'HasField', 'IsInitialized', 'ListFields', 'MergeFrom', 'MergeFromString', 'ParseFromString', 'RegisterExtension', 'SerializePartialToString', 'SerializeToString', 'SetInParent', 'UnknownFields', 'WhichOneof', '_CheckCalledFromGeneratedFile', '_SetListener', '__class__', '__deepcopy__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__unicode__', 'associated_detections', 'detection_id', 'display_name', 'feature_tag', 'label', 'label_id', 'location_data', 'score', 'timestamp_usec', 'track_id']\n",
      "\n",
      "[score Í∞í]\n",
      "[0.8923986554145813]\n",
      "\n",
      "[location_data ÏÜçÏÑ±]\n",
      "['BOUNDING_BOX', 'BinaryMask', 'BoundingBox', 'ByteSize', 'Clear', 'ClearExtension', 'ClearField', 'CopyFrom', 'DESCRIPTOR', 'DiscardUnknownFields', 'Extensions', 'FindInitializationErrors', 'Format', 'FromString', 'GLOBAL', 'HasExtension', 'HasField', 'IsInitialized', 'ListFields', 'MASK', 'MergeFrom', 'MergeFromString', 'ParseFromString', 'RELATIVE_BOUNDING_BOX', 'RegisterExtension', 'RelativeBoundingBox', 'RelativeKeypoint', 'SerializePartialToString', 'SerializeToString', 'SetInParent', 'UnknownFields', 'WhichOneof', '_CheckCalledFromGeneratedFile', '_SetListener', '__class__', '__deepcopy__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__unicode__', 'bounding_box', 'format', 'mask', 'relative_bounding_box', 'relative_keypoints']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1757487859.451876   64920 gl_context.cc:369] GL version: 2.1 (2.1 ATI-7.0.23), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1757487859.455780   77447 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "# BGR ‚Üí RGB\n",
    "image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "\n",
    "    results = face_detection.process(image_rgb)\n",
    "\n",
    "    print(\"\\n[results Í∞ùÏ≤¥ ÏÜçÏÑ±]\")\n",
    "    print(dir(results))\n",
    "\n",
    "    if results.detections:\n",
    "        detection = results.detections[0]\n",
    "        print(\"\\n[Ï≤´ Î≤àÏß∏ detection ÏÜçÏÑ±]\")\n",
    "        print(dir(detection))\n",
    "\n",
    "        print(\"\\n[score Í∞í]\")\n",
    "        print(detection.score)\n",
    "\n",
    "        print(\"\\n[location_data ÏÜçÏÑ±]\")\n",
    "        print(dir(detection.location_data))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
